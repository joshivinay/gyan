{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414731a4-06b0-40ee-90f8-06b36c6513f1",
   "metadata": {},
   "source": [
    "# Lakehouse Schema Evolution Rules\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b31be2ca-f26a-41c8-9f06-7a876a02b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "RESOURCE_PATH=\".\"\n",
    "ADD_COLUMNS_PATH = f\"{RESOURCE_PATH}/add_columns\"\n",
    "DROP_COLUMNS_PATH = f\"{RESOURCE_PATH}/drop_columns\"\n",
    "CHANGE_COLUMNS_PATH = f\"{RESOURCE_PATH}/change_column_types\"\n",
    "OUTPUT_PATH=\"./output\"\n",
    "ADD_COLUMNS_OUTPUT_PATH = f\"{OUTPUT_PATH}/add_columns\"\n",
    "DROP_COLUMNS_OUTPUT_PATH = f\"{OUTPUT_PATH}/drop_columns\"\n",
    "CHANGE_COLUMNS_OUTPUT_PATH = f\"{OUTPUT_PATH}/change_column_types\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e28c1ca-e114-4135-bfdd-70be1b733cc2",
   "metadata": {},
   "source": [
    "# Adding Data Columns (Part I)\n",
    "##  Add Columns in flat schema\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea7b7c9a-4ada-4bd2-abc0-e5ac63b184e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Schema shape of original data is \n",
      "---------------------------------\n",
      "root\n",
      " |-- description: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- record_number: long (nullable = true)\n",
      "\n",
      "-----------------------------------------\n",
      "Output when querying the original dataset\n",
      "-----------------------------------------\n",
      "+-------------+-----+------------------------------+\n",
      "|record_number|name |description                   |\n",
      "+-------------+-----+------------------------------+\n",
      "|1            |oftra|One framework to rule them all|\n",
      "+-------------+-----+------------------------------+\n",
      "\n",
      "---------------------------------------\n",
      "Schema shape of newer incoming data is \n",
      "---------------------------------------\n",
      "root\n",
      " |-- description: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- record_number: long (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      "\n",
      "---------------------------------------\n",
      "Merged schema of target delta table is \n",
      "---------------------------------------\n",
      "root\n",
      " |-- description: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- record_number: long (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      "\n",
      "-----------------------------------\n",
      "Output when querying merged dataset\n",
      "-----------------------------------\n",
      "+------------------------------+-----+-------------+--------+-------+\n",
      "|description                   |name |record_number|language|version|\n",
      "+------------------------------+-----+-------------+--------+-------+\n",
      "|One framework to rule them all|oftra|2            |Python  |3.11   |\n",
      "|One framework to rule them all|oftra|1            |null    |null   |\n",
      "+------------------------------+-----+-------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add Columns in flat schema\n",
    "flat_schema_df = spark.read.format(\"json\").option(\"multiLine\",\"true\") \\\n",
    "                      .load(f\"{ADD_COLUMNS_PATH}/flat_schema/flat_schema.json\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(\"Schema shape of original data is \")\n",
    "print(\"---------------------------------\")\n",
    "flat_schema_df.printSchema()\n",
    "flat_schema_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{ADD_COLUMNS_OUTPUT_PATH}/flat_schema.delta\")\n",
    "flat_schema_df_reread = spark.read.format(\"delta\").load(f\"{ADD_COLUMNS_OUTPUT_PATH}/flat_schema.delta\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"Output when querying the original dataset\")\n",
    "print(\"-----------------------------------------\")\n",
    "flat_schema_df_reread.select(\"record_number\",\"name\", \"description\").show(truncate=False)\n",
    "\n",
    "# Read in json data files with columns added to the flat schema\n",
    "flat_schema_df_latest = spark.read.format(\"json\").option(\"multiLine\",\"true\") \\\n",
    "                             .load(f\"{ADD_COLUMNS_PATH}/flat_schema/flat_schema_add_columns.json\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Schema shape of newer incoming data is \")\n",
    "print(\"---------------------------------------\")\n",
    "flat_schema_df_latest.printSchema()\n",
    "flat_schema_df_latest.write.format(\"delta\").mode(\"append\") \\\n",
    "                     .option(\"mergeSchema\", \"true\").save(f\"{ADD_COLUMNS_OUTPUT_PATH}/flat_schema.delta\")\n",
    "flat_schema_df_latest_reread = spark.read.format(\"delta\").load(f\"{ADD_COLUMNS_OUTPUT_PATH}/flat_schema.delta\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Merged schema of target delta table is \")\n",
    "print(\"---------------------------------------\")\n",
    "flat_schema_df_latest_reread.printSchema()\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Output when querying merged dataset\")\n",
    "print(\"-----------------------------------\")\n",
    "flat_schema_df_latest_reread.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e35996-5cce-40dd-9090-2cdb3b1987eb",
   "metadata": {},
   "source": [
    "# Adding Data Columns (Part I)\n",
    "## Add Columns inside struct schema\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98a2c6eb-407d-40b9-9394-560e0ee68dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Schema shape of original data is \n",
      "---------------------------------\n",
      "root\n",
      " |-- project: struct (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- record_number: long (nullable = true)\n",
      "\n",
      "-----------------------------------------\n",
      "Output when querying the original dataset\n",
      "-----------------------------------------\n",
      "+------------------------------------------+\n",
      "|project                                   |\n",
      "+------------------------------------------+\n",
      "|{One framework to rule them all, oftra, 1}|\n",
      "+------------------------------------------+\n",
      "\n",
      "---------------------------------------\n",
      "Schema shape of newer incoming data is \n",
      "---------------------------------------\n",
      "root\n",
      " |-- project: struct (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- language: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- record_number: long (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      "\n",
      "---------------------------------------\n",
      "Merged schema of target delta table is \n",
      "---------------------------------------\n",
      "root\n",
      " |-- project: struct (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- record_number: long (nullable = true)\n",
      " |    |-- language: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      "\n",
      "-----------------------------------\n",
      "Output when querying merged dataset\n",
      "-----------------------------------\n",
      "+--------------------------------------------------+-------+\n",
      "|project                                           |version|\n",
      "+--------------------------------------------------+-------+\n",
      "|{One framework to rule them all, oftra, 2, Python}|3.11   |\n",
      "|{One framework to rule them all, oftra, 1, null}  |null   |\n",
      "+--------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add Columns inside struct schema\n",
    "struct_schema_df = spark.read.format(\"json\").option(\"multiLine\",\"true\") \\\n",
    "                        .load(f\"{ADD_COLUMNS_PATH}/struct_schema/struct_schema.json\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(\"Schema shape of original data is \")\n",
    "print(\"---------------------------------\")\n",
    "struct_schema_df.printSchema()\n",
    "struct_schema_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{ADD_COLUMNS_OUTPUT_PATH}/struct_schema.delta\")\n",
    "struct_schema_df_reread = spark.read.format(\"delta\").load(f\"{ADD_COLUMNS_OUTPUT_PATH}/struct_schema.delta\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"Output when querying the original dataset\")\n",
    "print(\"-----------------------------------------\")\n",
    "struct_schema_df_reread.show(truncate=False)\n",
    "\n",
    "# Read in json data with added columns to the struct schema\n",
    "struct_schema_df_latest = spark.read.format(\"json\").option(\"multiLine\",\"true\") \\\n",
    "                        .load(f\"{ADD_COLUMNS_PATH}/struct_schema/struct_schema_add_columns.json\")\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Schema shape of newer incoming data is \")\n",
    "print(\"---------------------------------------\")\n",
    "struct_schema_df_latest.printSchema()\n",
    "struct_schema_df_latest.write.format(\"delta\").mode(\"append\") \\\n",
    "                       .option(\"mergeSchema\", \"true\") \\\n",
    "                       .save(f\"{ADD_COLUMNS_OUTPUT_PATH}/struct_schema.delta\")\n",
    "struct_schema_df_latest_reread = spark.read.format(\"delta\").load(f\"{ADD_COLUMNS_OUTPUT_PATH}/struct_schema.delta\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Merged schema of target delta table is \")\n",
    "print(\"---------------------------------------\")\n",
    "struct_schema_df_latest_reread.printSchema()\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Output when querying merged dataset\")\n",
    "print(\"-----------------------------------\")\n",
    "struct_schema_df_latest_reread.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e7299d-c4f0-4316-abd4-3327408cda92",
   "metadata": {},
   "source": [
    "# Adding Data Columns (Part I)\n",
    "## Add Columns in struct nested in another struct or nested inside an array\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e18eba6-1465-46e1-baf2-e7bc0de243fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Schema shape of original data is \n",
      "---------------------------------\n",
      "root\n",
      " |-- projects: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- language: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- record_number: long (nullable = true)\n",
      "\n",
      "-----------------------------------------\n",
      "Output when querying the original dataset\n",
      "-----------------------------------------\n",
      "+------------------+\n",
      "|project           |\n",
      "+------------------+\n",
      "|{Python, oftra, 5}|\n",
      "|{Python, oftra, 6}|\n",
      "+------------------+\n",
      "\n",
      "---------------------------------------\n",
      "Schema shape of newer incoming data is \n",
      "---------------------------------------\n",
      "root\n",
      " |-- projects: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- language: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- record_number: long (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      "\n",
      "---------------------------------------\n",
      "Merged schema of target delta table is \n",
      "---------------------------------------\n",
      "root\n",
      " |-- projects: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- language: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- record_number: long (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      "\n",
      "--------------------------------------------------------\n",
      "Exploded Array Output when querying the original dataset\n",
      "--------------------------------------------------------\n",
      "+--------------------------------------------------+-------+\n",
      "|project                                           |version|\n",
      "+--------------------------------------------------+-------+\n",
      "|{Python, oftra, 3, One framework to rule them all}|3.11   |\n",
      "|{Python, oftra, 4, One framework to rule them all}|3.11   |\n",
      "|{Python, oftra, 5, null}                          |null   |\n",
      "|{Python, oftra, 6, null}                          |null   |\n",
      "+--------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add Columns in a struct schema inside an array\n",
    "from pyspark.sql.functions import explode\n",
    "ordered_df = spark.read.format(\"json\").option(\"multiLine\",\"true\") \\\n",
    "                  .load(f\"{ADD_COLUMNS_PATH}/struct_schema/struct_schema_inside_array.json\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(\"Schema shape of original data is \")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "ordered_df.printSchema()\n",
    "ordered_df.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "          .save(f\"{ADD_COLUMNS_OUTPUT_PATH}/struct_schema_inside_array.delta\")\n",
    "struct_schema_df_reread = spark.read.format(\"delta\") \\\n",
    "                               .load(f\"{ADD_COLUMNS_OUTPUT_PATH}/struct_schema_inside_array.delta\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"Output when querying the original dataset\")\n",
    "print(\"-----------------------------------------\")\n",
    "struct_schema_df_reread.select(explode(\"projects\").alias(\"project\")).show(truncate=False)\n",
    "\n",
    "# Read in some json files with a couple of added columns to the flat schema\n",
    "struct_schema_df_latest = spark.read.format(\"json\").option(\"multiLine\",\"true\") \\\n",
    "                               .load(f\"{ADD_COLUMNS_PATH}/struct_schema/struct_schema_inside_array_add_columns.json\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Schema shape of newer incoming data is \")\n",
    "print(\"---------------------------------------\")\n",
    "struct_schema_df_latest.printSchema()\n",
    "struct_schema_df_latest.write.format(\"delta\").mode(\"append\") \\\n",
    "                       .option(\"mergeSchema\", \"true\") \\\n",
    "                       .save(f\"{ADD_COLUMNS_OUTPUT_PATH}/struct_schema_inside_array.delta\")\n",
    "struct_schema_df_latest_reread = spark.read.format(\"delta\") \\\n",
    "                                      .load(f\"{ADD_COLUMNS_OUTPUT_PATH}/struct_schema_inside_array.delta\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Merged schema of target delta table is \")\n",
    "print(\"---------------------------------------\")\n",
    "struct_schema_df_latest_reread.printSchema()\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Exploded Array Output when querying the original dataset\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "struct_schema_df_latest_reread.select(explode(\"projects\").alias(\"project\"), \"version\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b208d-7395-4d19-86eb-7e27b1deb092",
   "metadata": {},
   "source": [
    "# Adding Data Columns (Part I)\n",
    "## Add Columns in array of arrays\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cef874a8-664c-447b-8419-b429391cfd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Schema shape of original data is \n",
      "---------------------------------\n",
      "root\n",
      " |-- matrix: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      "\n",
      "-----------------------------------------\n",
      "Output when querying the original dataset\n",
      "-----------------------------------------\n",
      "+--------------------+\n",
      "|matrix              |\n",
      "+--------------------+\n",
      "|[[10, 20], [30, 40]]|\n",
      "|[[50, 60], [70, 80]]|\n",
      "+--------------------+\n",
      "\n",
      "---------------------------------------\n",
      "Schema shape of newer incoming data is \n",
      "---------------------------------------\n",
      "root\n",
      " |-- matrix: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Failed to merge fields 'matrix' and 'matrix'. Failed to merge incompatible data types LongType and ArrayType(LongType,true)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m df_latest\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[1;32m     26\u001b[0m \u001b[43mdf_latest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 27\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmergeSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mADD_COLUMNS_OUTPUT_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/array_inside_array.delta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m df_latest_reread \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mADD_COLUMNS_OUTPUT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/array_inside_array.delta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Applications/spark/python/pyspark/sql/readwriter.py:1398\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1398\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Applications/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to merge fields 'matrix' and 'matrix'. Failed to merge incompatible data types LongType and ArrayType(LongType,true)"
     ]
    }
   ],
   "source": [
    "# Adding columns inside an array of array of arrays\n",
    "from pyspark.sql.functions import explode\n",
    "df = spark.read.format(\"json\").option(\"multiLine\",\"true\").load(f\"{ADD_COLUMNS_PATH}/array_schema/matrix.json\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(\"Schema shape of original data is \")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "df.printSchema()\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(f\"{ADD_COLUMNS_OUTPUT_PATH}/array_inside_array.delta\")\n",
    "df_reread = spark.read.format(\"delta\").load(f\"{ADD_COLUMNS_OUTPUT_PATH}/array_inside_array.delta\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"Output when querying the original dataset\")\n",
    "print(\"-----------------------------------------\")\n",
    "df_reread.select(explode(\"matrix\").alias(\"matrix\")).show(truncate=False)\n",
    "\n",
    "# Read in json data with added array inside array of arrays\n",
    "df_latest = spark.read.format(\"json\").option(\"multiLine\",\"true\") \\\n",
    "                        .load(f\"{ADD_COLUMNS_PATH}/array_schema/tensor.json\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Schema shape of newer incoming data is \")\n",
    "print(\"---------------------------------------\")\n",
    "df_latest.printSchema()\n",
    "df_latest.write.format(\"delta\").mode(\"append\") \\\n",
    "                     .option(\"mergeSchema\", \"true\").save(f\"{ADD_COLUMNS_OUTPUT_PATH}/array_inside_array.delta\")\n",
    "df_latest_reread = spark.read.format(\"delta\").load(f\"{ADD_COLUMNS_OUTPUT_PATH}/array_inside_array.delta\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Merged schema of target delta table is \")\n",
    "print(\"---------------------------------------\")\n",
    "df_latest_reread.printSchema()\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Output when querying merged dataset\")\n",
    "print(\"-----------------------------------\")\n",
    "df_latest_reread.select(explode(\"matrix\").alias(\"matrix\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aacfa96-b073-4a51-aa58-04e896f3ba13",
   "metadata": {},
   "source": [
    "# Drop Columns (Part II)\n",
    "## Drop Columns in a flat schema\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3a9e02d-4865-4c0d-ae84-42351a81e5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Schema shape of original data is \n",
      "---------------------------------\n",
      "root\n",
      " |-- record_number: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      "\n",
      "-----------------------------------------\n",
      "Output when querying the original dataset\n",
      "-----------------------------------------\n",
      "+-------------+-----+------------------------------+--------+-------+\n",
      "|record_number|name |description                   |language|version|\n",
      "+-------------+-----+------------------------------+--------+-------+\n",
      "|1            |oftra|One framework to rule them all|Python  |3.11   |\n",
      "+-------------+-----+------------------------------+--------+-------+\n",
      "\n",
      "---------------------------------------\n",
      "Schema shape of newer incoming data is \n",
      "---------------------------------------\n",
      "root\n",
      " |-- record_number: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "---------------------------------------\n",
      "Merged schema of target delta table is \n",
      "---------------------------------------\n",
      "root\n",
      " |-- record_number: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      "\n",
      "-----------------------------------\n",
      "Output when querying merged dataset\n",
      "-----------------------------------\n",
      "+-------------+-----+------------------------------+--------+-------+\n",
      "|record_number|name |description                   |language|version|\n",
      "+-------------+-----+------------------------------+--------+-------+\n",
      "|1            |oftra|One framework to rule them all|Python  |3.11   |\n",
      "|2            |oftra|One framework to rule them all|null    |null   |\n",
      "+-------------+-----+------------------------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop Columns in a flat schema\n",
    "flat_schema_df = spark.read.format(\"json\").option(\"multiLine\",\"true\") \\\n",
    "                      .load(f\"{DROP_COLUMNS_PATH}/flat_schema/flat_schema.json\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(\"Schema shape of original data is \")\n",
    "print(\"---------------------------------\")\n",
    "ordered_df = flat_schema_df.select(\"record_number\",\"name\",\"description\",\"language\",\"version\")\n",
    "ordered_df.printSchema()\n",
    "ordered_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{DROP_COLUMNS_OUTPUT_PATH}/flat_schema.delta\")\n",
    "\n",
    "flat_schema_df_reread = spark.read.format(\"delta\").load(f\"{DROP_COLUMNS_OUTPUT_PATH}/flat_schema.delta\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"Output when querying the original dataset\")\n",
    "print(\"-----------------------------------------\")\n",
    "flat_schema_df_reread.show(truncate=False)\n",
    "\n",
    "# Read in json data with a couple of deleted columns to the flat schema\n",
    "flat_schema_df_latest = spark.read.format(\"json\") \\\n",
    "                             .schema(\"record_number long, name string, description string\") \\\n",
    "                             .option(\"multiLine\",\"true\") \\\n",
    "                             .load(f\"{DROP_COLUMNS_PATH}/flat_schema/flat_schema_drop_columns.json\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Schema shape of newer incoming data is \")\n",
    "print(\"---------------------------------------\")\n",
    "flat_schema_df_latest.printSchema()\n",
    "flat_schema_df_latest.write.format(\"delta\").mode(\"append\") \\\n",
    "                     .option(\"mergeSchema\", \"true\").save(f\"{DROP_COLUMNS_OUTPUT_PATH}/flat_schema.delta\")\n",
    "flat_schema_df_latest_reread = spark.read.format(\"delta\").load(f\"{DROP_COLUMNS_OUTPUT_PATH}/flat_schema.delta\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Merged schema of target delta table is \")\n",
    "print(\"---------------------------------------\")\n",
    "flat_schema_df_latest_reread.printSchema()\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Output when querying merged dataset\")\n",
    "print(\"-----------------------------------\")\n",
    "flat_schema_df_latest_reread.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9fd527-3df7-4475-9dcd-637e68924248",
   "metadata": {},
   "source": [
    "# Drop Columns (Part II)\n",
    "## Drop Columns in a struct schema\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a9a162b-41d0-4e20-8855-637ca3331720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Schema shape of original data is \n",
      "---------------------------------\n",
      "root\n",
      " |-- project: struct (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- language: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- record_number: long (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      "\n",
      "-----------------------------------------\n",
      "Output when querying the original dataset\n",
      "-----------------------------------------\n",
      "+----------------------------------------+\n",
      "|projects                                |\n",
      "+----------------------------------------+\n",
      "|[{Python, oftra, 3}, {Python, oftra, 4}]|\n",
      "|[{Python, oftra, 5}, {Python, oftra, 6}]|\n",
      "+----------------------------------------+\n",
      "\n",
      "---------------------------------------\n",
      "Schema shape of newer incoming data is \n",
      "---------------------------------------\n",
      "root\n",
      " |-- project: struct (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- record_number: long (nullable = true)\n",
      "\n",
      "---------------------------------------\n",
      "Merged schema of target delta table is \n",
      "---------------------------------------\n",
      "root\n",
      " |-- project: struct (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- language: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- record_number: long (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      "\n",
      "-----------------------------------\n",
      "Output when querying merged dataset\n",
      "-----------------------------------\n",
      "+--------------------------------------------------+-------+\n",
      "|project                                           |version|\n",
      "+--------------------------------------------------+-------+\n",
      "|{One framework to rule them all, Python, oftra, 2}|3.11   |\n",
      "|{One framework to rule them all, null, oftra, 1}  |null   |\n",
      "+--------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop Columns in a struct schema\n",
    "struct_schema_df = spark.read.format(\"json\").option(\"multiLine\",\"true\").load(f\"{DROP_COLUMNS_PATH}/struct_schema/struct_schema.json\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(\"Schema shape of original data is \")\n",
    "print(\"---------------------------------\")\n",
    "struct_schema_df.printSchema()\n",
    "struct_schema_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{DROP_COLUMNS_OUTPUT_PATH}/struct_schema.delta\")\n",
    "struct_schema_df = spark.read.format(\"delta\").load(f\"{DROP_COLUMNS_OUTPUT_PATH}/struct_schema.delta\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"Output when querying the original dataset\")\n",
    "print(\"-----------------------------------------\")\n",
    "struct_schema_df_reread.show(truncate=False)\n",
    "\n",
    "# Read in some json files with a couple of deleted columns to the flat schema\n",
    "struct_schema_df_latest = spark.read.format(\"json\").option(\"multiLine\",\"true\") \\\n",
    "                        .load(f\"{DROP_COLUMNS_PATH}/struct_schema/struct_schema_drop_columns.json\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Schema shape of newer incoming data is \")\n",
    "print(\"---------------------------------------\")\n",
    "struct_schema_df_latest.printSchema()\n",
    "struct_schema_df_latest.write.format(\"delta\").mode(\"append\") \\\n",
    "                     .option(\"mergeSchema\", \"true\").save(f\"{DROP_COLUMNS_OUTPUT_PATH}/struct_schema.delta\")\n",
    "struct_schema_df_latest_reread = spark.read.format(\"delta\").load(f\"{DROP_COLUMNS_OUTPUT_PATH}/struct_schema.delta\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Merged schema of target delta table is \")\n",
    "print(\"---------------------------------------\")\n",
    "struct_schema_df_latest_reread.printSchema()\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Output when querying merged dataset\")\n",
    "print(\"-----------------------------------\")\n",
    "struct_schema_df_latest_reread.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3641cd-d791-4afb-84f1-b609449004fd",
   "metadata": {},
   "source": [
    "# Drop Columns (Part II)\n",
    "## Drop Columns in a struct schema inside an array\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7971f8b5-1e6f-4ab8-bb94-f44af1c1aa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Schema shape of original data is \n",
      "---------------------------------\n",
      "root\n",
      " |-- projects: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- language: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- record_number: long (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      "\n",
      "-----------------------------------------\n",
      "Output when querying the original dataset\n",
      "-----------------------------------------\n",
      "+--------------------------------------------------+-------+\n",
      "|project                                           |version|\n",
      "+--------------------------------------------------+-------+\n",
      "|{One framework to rule them all, Python, oftra, 3}|3.11   |\n",
      "|{One framework to rule them all, Python, oftra, 4}|3.11   |\n",
      "+--------------------------------------------------+-------+\n",
      "\n",
      "---------------------------------------\n",
      "Schema shape of newer incoming data is \n",
      "---------------------------------------\n",
      "root\n",
      " |-- projects: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- language: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- record_number: long (nullable = true)\n",
      "\n",
      "---------------------------------------\n",
      "Merged schema of target delta table is \n",
      "---------------------------------------\n",
      "root\n",
      " |-- projects: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- language: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- record_number: long (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      "\n",
      "-----------------------------------\n",
      "Output when querying merged dataset\n",
      "-----------------------------------\n",
      "+--------------------------------------------------+-------+\n",
      "|project                                           |version|\n",
      "+--------------------------------------------------+-------+\n",
      "|{One framework to rule them all, Python, oftra, 3}|3.11   |\n",
      "|{One framework to rule them all, Python, oftra, 4}|3.11   |\n",
      "|{null, Python, oftra, 5}                          |null   |\n",
      "|{null, Python, oftra, 6}                          |null   |\n",
      "+--------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop Columns in a struct schema inside an array\n",
    "from pyspark.sql.functions import explode\n",
    "ordered_df = spark.read.format(\"json\").option(\"multiLine\",\"true\").load(f\"{DROP_COLUMNS_PATH}/struct_schema/struct_schema_inside_array.json\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(\"Schema shape of original data is \")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "ordered_df.printSchema()\n",
    "ordered_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{DROP_COLUMNS_OUTPUT_PATH}/struct_schema_inside_array.delta\")\n",
    "struct_schema_df_reread = spark.read.format(\"delta\").load(f\"{DROP_COLUMNS_OUTPUT_PATH}/struct_schema_inside_array.delta\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"Output when querying the original dataset\")\n",
    "print(\"-----------------------------------------\")\n",
    "struct_schema_df_reread.select(explode(\"projects\").alias(\"project\"), \"version\").show(truncate=False)\n",
    "\n",
    "# Read in some json files with a couple of deleted columns to the flat schema\n",
    "struct_schema_df_latest = spark.read.format(\"json\").option(\"multiLine\",\"true\") \\\n",
    "                        .load(f\"{DROP_COLUMNS_PATH}/struct_schema/struct_schema_inside_array_drop_columns.json\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Schema shape of newer incoming data is \")\n",
    "print(\"---------------------------------------\")\n",
    "struct_schema_df_latest.printSchema()\n",
    "struct_schema_df_latest.write.format(\"delta\").mode(\"append\") \\\n",
    "                     .option(\"mergeSchema\", \"true\").save(f\"{DROP_COLUMNS_OUTPUT_PATH}/struct_schema_inside_array.delta\")\n",
    "struct_schema_df_latest_reread = spark.read.format(\"delta\").load(f\"{DROP_COLUMNS_OUTPUT_PATH}/struct_schema_inside_array.delta\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Merged schema of target delta table is \")\n",
    "print(\"---------------------------------------\")\n",
    "struct_schema_df_latest_reread.printSchema()\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Output when querying merged dataset\")\n",
    "print(\"-----------------------------------\")\n",
    "struct_schema_df_latest_reread.select(explode(\"projects\").alias(\"project\"), \"version\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df0cad-08b2-4ce7-97d1-1396ce06fb39",
   "metadata": {},
   "source": [
    "# Drop Columns (Part II)\n",
    "## Drop Columns in free form array of arrays\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db2c7735-2a59-42ef-9475-23b96e75b31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Schema shape of original data is \n",
      "---------------------------------\n",
      "root\n",
      " |-- matrix: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      "\n",
      "-----------------------------------------\n",
      "Output when querying the original dataset\n",
      "-----------------------------------------\n",
      "+-------------------------------------------+\n",
      "|matrix                                     |\n",
      "+-------------------------------------------+\n",
      "|[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]       |\n",
      "|[[[9, 10], [11, 12]], [[13, 14], [15, 16]]]|\n",
      "+-------------------------------------------+\n",
      "\n",
      "---------------------------------------\n",
      "Schema shape of newer incoming data is \n",
      "---------------------------------------\n",
      "root\n",
      " |-- matrix: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Failed to merge fields 'matrix' and 'matrix'. Failed to merge incompatible data types ArrayType(LongType,true) and LongType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m df_latest\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[1;32m     27\u001b[0m \u001b[43mdf_latest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 28\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmergeSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDROP_COLUMNS_OUTPUT_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/array_inside_array.delta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m df_latest_reread \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDROP_COLUMNS_OUTPUT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/array_inside_array.delta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Applications/spark/python/pyspark/sql/readwriter.py:1398\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1398\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Applications/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to merge fields 'matrix' and 'matrix'. Failed to merge incompatible data types ArrayType(LongType,true) and LongType"
     ]
    }
   ],
   "source": [
    "# Drop Columns in free form array of arrays\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df = spark.read.format(\"json\").option(\"multiLine\",\"true\").load(f\"{DROP_COLUMNS_PATH}/array_schema/tensor.json\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(\"Schema shape of original data is \")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "df.printSchema()\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(f\"{DROP_COLUMNS_OUTPUT_PATH}/array_inside_array.delta\")\n",
    "df_reread = spark.read.format(\"delta\").load(f\"{DROP_COLUMNS_OUTPUT_PATH}/array_inside_array.delta\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"Output when querying the original dataset\")\n",
    "print(\"-----------------------------------------\")\n",
    "df_reread.select(explode(\"matrix\").alias(\"matrix\")).show(truncate=False)\n",
    "\n",
    "# Read in some json files with a couple of deleted columns in the array of array\n",
    "df_latest = spark.read.format(\"json\").option(\"multiLine\",\"true\") \\\n",
    "                        .load(f\"{DROP_COLUMNS_PATH}/array_schema/matrix.json\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Schema shape of newer incoming data is \")\n",
    "print(\"---------------------------------------\")\n",
    "df_latest.printSchema()\n",
    "df_latest.write.format(\"delta\").mode(\"append\") \\\n",
    "                     .option(\"mergeSchema\", \"true\").save(f\"{DROP_COLUMNS_OUTPUT_PATH}/array_inside_array.delta\")\n",
    "df_latest_reread = spark.read.format(\"delta\").load(f\"{DROP_COLUMNS_OUTPUT_PATH}/array_inside_array.delta\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Merged schema of target delta table is \")\n",
    "print(\"---------------------------------------\")\n",
    "df_latest_reread.printSchema()\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Output when querying merged dataset\")\n",
    "print(\"-----------------------------------\")\n",
    "df_latest_reread.select(explode(\"matrix\").alias(\"matrix\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac11ed9e-4618-49ae-a4ee-b43519b0a8ce",
   "metadata": {},
   "source": [
    "# Change Column Data Types (Part III)\n",
    "## Change column types in flat schema\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59d79ee4-101a-4eb1-a1cd-38719d417747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Schema shape of original data is \n",
      "---------------------------------\n",
      "root\n",
      " |-- rn_byte_short: byte (nullable = true)\n",
      " |-- rn_byte_int: byte (nullable = true)\n",
      " |-- rn_short_int: short (nullable = true)\n",
      " |-- rn_int_long: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "-----------------------------------------\n",
      "Output when querying the original dataset\n",
      "-----------------------------------------\n",
      "+-------------+-----------+------------+-----------+-----+-----------+\n",
      "|rn_byte_short|rn_byte_int|rn_short_int|rn_int_long|name |description|\n",
      "+-------------+-----------+------------+-----------+-----+-----------+\n",
      "|1            |1          |200         |1          |oftra|Only oftra |\n",
      "+-------------+-----------+------------+-----------+-----+-----------+\n",
      "\n",
      "---------------------------------------\n",
      "Schema shape of newer incoming data is \n",
      "---------------------------------------\n",
      "root\n",
      " |-- rn_byte_short: short (nullable = true)\n",
      " |-- rn_byte_int: integer (nullable = true)\n",
      " |-- rn_short_int: integer (nullable = true)\n",
      " |-- rn_int_long: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "---------------------------------------\n",
      "Merged schema of target delta table is \n",
      "---------------------------------------\n",
      "root\n",
      " |-- rn_byte_short: short (nullable = true)\n",
      " |-- rn_byte_int: integer (nullable = true)\n",
      " |-- rn_short_int: integer (nullable = true)\n",
      " |-- rn_int_long: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "-----------------------------------\n",
      "Output when querying merged dataset\n",
      "-----------------------------------\n",
      "+-------------+-----------+------------+-----------+-----+-----------+\n",
      "|rn_byte_short|rn_byte_int|rn_short_int|rn_int_long|name |description|\n",
      "+-------------+-----------+------------+-----------+-----+-----------+\n",
      "|1            |1          |200         |1          |oftra|Only oftra |\n",
      "|200          |63000      |63000       |100000000  |oftra|Only Oftra |\n",
      "+-------------+-----------+------------+-----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change column types in flat schema\n",
    "\n",
    "flat_schema = f\"rn_byte_short byte, rn_byte_int byte, rn_short_int short, \\\n",
    "                         rn_int_long long, name string, description string\"\n",
    "flat_schema_df = spark.read.format(\"json\") \\\n",
    "                      .schema(flat_schema) \\\n",
    "                      .option(\"multiLine\",\"true\") \\\n",
    "                      .load(f\"{CHANGE_COLUMNS_PATH}/flat_schema/flat_schema.json\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(\"Schema shape of original data is \")\n",
    "print(\"---------------------------------\")\n",
    "flat_schema_df.printSchema()\n",
    "flat_schema_df.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "              .save(f\"{CHANGE_COLUMNS_OUTPUT_PATH}/flat_schema.delta\")\n",
    "flat_schema_df_reread = spark.read.format(\"delta\") \\\n",
    "                             .load(f\"{CHANGE_COLUMNS_OUTPUT_PATH}/flat_schema.delta\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"Output when querying the original dataset\")\n",
    "print(\"-----------------------------------------\")\n",
    "flat_schema_df_reread.show(truncate=False)\n",
    "\n",
    "# Read in json files with widened types\n",
    "widened_schema = f\"rn_byte_short short, rn_byte_int int, rn_short_int integer, \\\n",
    "                         rn_int_long long, name string, description string\"\n",
    "                         \n",
    "flat_schema_df_latest = spark.read.format(\"json\") \\\n",
    "                        .schema(widened_schema) \\\n",
    "                        .option(\"multiLine\",\"true\") \\\n",
    "                        .load(f\"{CHANGE_COLUMNS_PATH}/flat_schema/flat_schema_change_column_types.json\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Schema shape of newer incoming data is \")\n",
    "print(\"---------------------------------------\")\n",
    "flat_schema_df_latest.printSchema()\n",
    "flat_schema_df_latest.write.format(\"delta\").mode(\"append\") \\\n",
    "                     .option(\"mergeSchema\", \"true\").save(f\"{CHANGE_COLUMNS_OUTPUT_PATH}/flat_schema.delta\")\n",
    "flat_schema_df_latest_reread = spark.read.format(\"delta\").load(f\"{CHANGE_COLUMNS_OUTPUT_PATH}/flat_schema.delta\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Merged schema of target delta table is \")\n",
    "print(\"---------------------------------------\")\n",
    "flat_schema_df_latest_reread.printSchema()\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Output when querying merged dataset\")\n",
    "print(\"-----------------------------------\")\n",
    "flat_schema_df_latest_reread.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59ced18-c04e-40f4-81cc-7f2eb27c3445",
   "metadata": {},
   "source": [
    "# Change Column Data Types (Part III)\n",
    "## Change column types in struct schema\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cd103c-aee4-44d4-8676-96641aea3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column types in struct schema\n",
    "struct_schema = f\"project STRUCT<rn_byte_short:byte, rn_byte_int:byte, rn_short_int:short, \\\n",
    "                         rn_int_long:long, name:string, description:string>\"\n",
    "struct_schema_df = spark.read.format(\"json\") \\\n",
    "                        .schema(struct_schema) \\\n",
    "                        .option(\"multiLine\",\"true\") \\\n",
    "                        .load(f\"{CHANGE_COLUMNS_PATH}/struct_schema/struct_schema.json\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(\"Schema shape of original data is \")\n",
    "print(\"---------------------------------\")\n",
    "struct_schema_df.printSchema()\n",
    "struct_schema_df.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "                .save(f\"{CHANGE_COLUMNS_OUTPUT_PATH}/struct_schema.delta\")\n",
    "struct_schema_df_reread = spark.read.format(\"delta\") \\\n",
    "                               .load(f\"{CHANGE_COLUMNS_OUTPUT_PATH}/struct_schema.delta\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"Output when querying the original dataset\")\n",
    "print(\"-----------------------------------------\")\n",
    "struct_schema_df_reread.show(truncate=False)\n",
    "\n",
    "# Read in json files with widened types\n",
    "widened_schema = f\"project STRUCT<rn_byte_short:short, rn_byte_int:int, rn_short_int:integer, \\\n",
    "                         rn_int_long:long, name:string, description:string>\"\n",
    "                         \n",
    "struct_schema_df_latest = spark.read.format(\"json\") \\\n",
    "                        .schema(widened_schema) \\\n",
    "                        .option(\"multiLine\",\"true\") \\\n",
    "                        .load(f\"{CHANGE_COLUMNS_PATH}/struct_schema/struct_schema_change_column_types.json\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Schema shape of newer incoming data is \")\n",
    "print(\"---------------------------------------\")\n",
    "struct_schema_df_latest.printSchema()\n",
    "struct_schema_df_latest.write.format(\"delta\").mode(\"append\") \\\n",
    "                     .option(\"mergeSchema\", \"true\").save(f\"{CHANGE_COLUMNS_OUTPUT_PATH}/struct_schema.delta\")\n",
    "struct_schema_df_latest_reread = spark.read.format(\"delta\").load(f\"{CHANGE_COLUMNS_OUTPUT_PATH}/struct_schema.delta\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Merged schema of target delta table is \")\n",
    "print(\"---------------------------------------\")\n",
    "struct_schema_df_latest_reread.printSchema()\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Output when querying merged dataset\")\n",
    "print(\"-----------------------------------\")\n",
    "struct_schema_df_latest_reread.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf000295-7877-4108-b6d4-391df6cf1b21",
   "metadata": {},
   "source": [
    "# Change Column Data Types (Part III)\n",
    "## Change column types in struct inside array schema\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f1554-037d-440f-9e94-e2602009c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column types in struct inside array schema\n",
    "from pyspark.sql.functions import explode, col\n",
    "struct_schema = f\"projects ARRAY<STRUCT<rn_byte_short:byte, rn_byte_int:byte, rn_short_int:short, \\\n",
    "                         rn_int_long:long, name:string, description:string>>\"\n",
    "struct_schema_df = spark.read.format(\"json\") \\\n",
    "                        .schema(struct_schema) \\\n",
    "                        .option(\"multiLine\",\"true\") \\\n",
    "                        .load(f\"{CHANGE_COLUMNS_PATH}/array_schema/struct_in_array_schema.json\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(\"Schema shape of original data is \")\n",
    "print(\"---------------------------------\")\n",
    "struct_schema_df.printSchema()\n",
    "struct_schema_df.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "                .save(f\"{CHANGE_COLUMNS_OUTPUT_PATH}/struct_in_array_schema.delta\")\n",
    "struct_schema_df_reread = spark.read.format(\"delta\") \\\n",
    "                               .load(f\"{CHANGE_COLUMNS_OUTPUT_PATH}/struct_in_array_schema.delta\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"Output when querying the original dataset\")\n",
    "print(\"-----------------------------------------\")\n",
    "struct_schema_df_reread.select(explode(\"projects\").alias(\"project\")).show(truncate=False)\n",
    "\n",
    "# Read in json files with widened types\n",
    "widened_schema = f\"projects ARRAY<STRUCT<rn_byte_short:short, rn_byte_int:int, rn_short_int:integer, \\\n",
    "                         rn_int_long:long, name:string, description:string>>\"\n",
    "                         \n",
    "struct_schema_df_latest = spark.read.format(\"json\") \\\n",
    "                        .schema(widened_schema) \\\n",
    "                        .option(\"multiLine\",\"true\") \\\n",
    "                        .load(f\"{CHANGE_COLUMNS_PATH}/array_schema/struct_in_array_change_colum_types.json\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Schema shape of newer incoming data is \")\n",
    "print(\"---------------------------------------\")\n",
    "struct_schema_df_latest.printSchema()\n",
    "struct_schema_df_latest.write.format(\"delta\").mode(\"append\") \\\n",
    "                       .option(\"mergeSchema\", \"true\") \\\n",
    "                       .save(f\"{CHANGE_COLUMNS_OUTPUT_PATH}/struct_in_array_schema.delta\")\n",
    "struct_schema_df_latest_reread = spark.read.format(\"delta\") \\\n",
    "                                      .load(f\"{CHANGE_COLUMNS_OUTPUT_PATH}/struct_in_array_schema.delta\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Merged schema of target delta table is \")\n",
    "print(\"---------------------------------------\")\n",
    "struct_schema_df_latest_reread.printSchema()\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Output when querying merged dataset\")\n",
    "print(\"-----------------------------------\")\n",
    "struct_schema_df_latest_reread.select(explode(\"projects\").alias(\"project\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eba4e9-b141-4762-afc6-ea0d0ce5cd21",
   "metadata": {},
   "source": [
    "# Change Column Data Types (Part III)\n",
    "## Change column types in array inside array schema\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b5c49-b7d0-4dc5-b0ae-1f9c822f1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column types in array inside array schema\n",
    "from pyspark.sql.functions import explode, col\n",
    "struct_schema = f\"matrix ARRAY<ARRAY<ARRAY<short>>>\"\n",
    "struct_schema_df = spark.read.format(\"json\") \\\n",
    "                .schema(struct_schema) \\\n",
    "                 .option(\"multiLine\",\"true\") \\\n",
    "                 .load(f\"{CHANGE_COLUMNS_PATH}/array_schema/matrix.json\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(\"Schema shape of original data is \")\n",
    "print(\"---------------------------------\")\n",
    "struct_schema_df.printSchema()\n",
    "struct_schema_df.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "                .save(f\"{CHANGE_COLUMNS_OUTPUT_PATH}/array_in_array_schema.delta\")\n",
    "struct_schema_df_reread = spark.read.format(\"delta\") \\\n",
    "                               .load(f\"{CHANGE_COLUMNS_OUTPUT_PATH}/array_in_array_schema.delta\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"Output when querying the original dataset\")\n",
    "print(\"-----------------------------------------\")\n",
    "struct_schema_df_reread.select(explode(\"matrix\").alias(\"matrix\")).show(truncate=False)\n",
    "\n",
    "# Read in json files with widened types\n",
    "widened_schema = f\"matrix ARRAY<ARRAY<ARRAY<int>>>\"\n",
    "                         \n",
    "struct_schema_df_latest = spark.read.format(\"json\") \\\n",
    "                               .schema(widened_schema) \\\n",
    "                               .option(\"multiLine\",\"true\") \\\n",
    "                               .load(f\"{CHANGE_COLUMNS_PATH}/array_schema/matrix_int.json\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Schema shape of newer incoming data is \")\n",
    "print(\"---------------------------------------\")\n",
    "struct_schema_df_latest.printSchema()\n",
    "struct_schema_df_latest.write.format(\"delta\").mode(\"append\") \\\n",
    "                       .option(\"mergeSchema\", \"true\") \\\n",
    "                       .save(f\"{CHANGE_COLUMNS_OUTPUT_PATH}/array_in_array_schema.delta\")\n",
    "struct_schema_df_latest_reread = spark.read.format(\"delta\") \\\n",
    "                                      .load(f\"{CHANGE_COLUMNS_OUTPUT_PATH}/array_in_array_schema.delta\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Merged schema of target delta table is \")\n",
    "print(\"---------------------------------------\")\n",
    "struct_schema_df_latest_reread.printSchema()\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Output when querying merged dataset\")\n",
    "print(\"-----------------------------------\")\n",
    "struct_schema_df_latest_reread.select(explode(\"matrix\").alias(\"matrix\")).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
